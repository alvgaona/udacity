{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the environment!\n",
    "\n",
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents.\n",
    "Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unity:\n",
    "    __FILE_NAME = 'Tennis_Linux/Tennis.x86_64'\n",
    "\n",
    "    def __init__(self):\n",
    "        self._env = UnityEnvironment(file_name=Unity.__FILE_NAME)\n",
    "        self._brain_name = self.env.brain_names[0]\n",
    "        self._brain = self.env.brains[self._brain_name]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def env(self):\n",
    "        return self._env\n",
    "\n",
    "    @property\n",
    "    def brain(self):\n",
    "        return self._brain\n",
    "\n",
    "    @property\n",
    "    def brain_name(self):\n",
    "        return self._brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "unity = Unity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Pick Actions\n",
    "\n",
    "In the next piece of code cell, controlling the agent and receiving feedback from the environment is shown.\n",
    "\n",
    "In the environment you started above, once the cell is executed, the agent will start interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class CrazyCompetition:\n",
    "    __MAX_TIMESTEPS = 2000\n",
    "\n",
    "    def __init__(self, unity, num_episodes, train_mode=True):\n",
    "        \"\"\"\n",
    "        Initialize the crazy agent.\n",
    "\n",
    "        Args:\n",
    "            unity (Unity): Unity wrapper for environment and brain.\n",
    "            num_episodes (int): Number of episodes to run.\n",
    "            train_mode (bool): Should be true if train mode is desired.\n",
    "        \"\"\"\n",
    "        env_info = unity.env.reset(train_mode=train_mode)[unity.brain_name]\n",
    "\n",
    "        self.unity = unity\n",
    "        self.train_mode = train_mode\n",
    "        self.num_episodes = num_episodes\n",
    "        self.action_size = unity.brain.vector_action_space_size\n",
    "        self.num_agents = len(env_info.agents)\n",
    "        self.states = env_info.vector_observations\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Train/Test the worst agent.\n",
    "\n",
    "        Note:\n",
    "            This agent is called crazy competition as it takes random actions to interact with the environment.\n",
    "            It does not learn from the environment as it does not use the states and rewards from it.\n",
    "            It just saves the reward and hopes it has a good one.\n",
    "\n",
    "            This is just for illustrative purposes.\n",
    "        \"\"\"\n",
    "        for i in range(0, self.num_episodes):\n",
    "            _ = self.unity.env.reset(train_mode=self.train_mode)[self.unity.brain_name]\n",
    "            scores = np.zeros(self.num_agents)\n",
    "\n",
    "            for t in range(CrazyCompetition.__MAX_TIMESTEPS):\n",
    "                actions = np.random.randn(self.num_agents, self.action_size)\n",
    "                actions = np.clip(actions, -1, 1)\n",
    "                env_info = self.unity.env.step(actions)[self.unity.brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                dones = env_info.local_done\n",
    "                scores += env_info.rewards\n",
    "                self.states = next_states\n",
    "\n",
    "                if np.any(dones):\n",
    "                    break\n",
    "\n",
    "            print(f'\\rEpisode {i}\\tScore (max over agents): {np.max(scores)}', end='')\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        ax.legend(['Score'])\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99\tScore (max over agents): 0.09000000171363354"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "crazy_competition = CrazyCompetition(unity, num_episodes, train_mode=False)\n",
    "crazy_competition.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize a replay buffer.\n",
    "\n",
    "        Note:\n",
    "            The data structure used to save the experiences is a queue of size `buffer_size`.\n",
    "            The queue is a queue of `namedtuples`.\n",
    "\n",
    "        Args:\n",
    "            action_size (int): Dimension of each action.\n",
    "            buffer_size (int): Maximum size of buffer.\n",
    "            batch_size (int): Size of each training batch.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to memory.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array):\n",
    "            action (numpy.array):\n",
    "            reward (float):\n",
    "            next_state (numpy.array):\n",
    "            done (bool): A flag to indicate if episode is done.\n",
    "        \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences from memory.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of internal memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, seed, mu, theta, sigma):\n",
    "        \"\"\"\n",
    "        Initialize parameters and noise process.\n",
    "\n",
    "        Args:\n",
    "            mu (float): Long-running mean.\n",
    "            theta (float): Speed of mean reversion.\n",
    "            sigma (float): Volatility parameter.\n",
    "        \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.state = copy.copy(self.mu)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the internal state (= noise) to mean (mu).\n",
    "        \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Update internal state and return it as a noise sample.\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def hidden_init(model):\n",
    "    \"\"\"\n",
    "    Computes the lower and upper bound of the uniform distribution.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): A model with weights to update.\n",
    "    \"\"\"\n",
    "    fan_in = model.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return -lim, lim\n",
    "\n",
    "def weights_init(model):\n",
    "    \"\"\"\n",
    "    Callback function to apply to a container model.\n",
    "\n",
    "    Note: Only initializes nn.Linear modules.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): A model with weights to update.\n",
    "    \"\"\"\n",
    "    if type(model) == nn.Linear:\n",
    "        model.weight.data.uniform_(*hidden_init(model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize actor class.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Dimension of the input size.\n",
    "            output_size (int): Dimension of the output size.\n",
    "            seed (int): Seed for random.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(16, output_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Apply a forward pass to the network.\n",
    "\n",
    "        Args:\n",
    "            state (nn.Tensor): Input of the network.\n",
    "        \"\"\"\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "\n",
    "        return self.model(state)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply weight initialization\n",
    "        \"\"\"\n",
    "        self.model.apply(weights_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize critic class.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Dimension of the input size.\n",
    "            seed (int): Seed for random.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Apply a forward pass to the network.\n",
    "\n",
    "        Args:\n",
    "            state (nn.Tensor): Input of the network.\n",
    "        \"\"\"\n",
    "        x = torch.cat((states, actions), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply weight initialization\n",
    "        \"\"\"\n",
    "        self.model.apply(weights_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, n_agents, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize actor-critic class.\n",
    "\n",
    "        Args:\n",
    "            n_agents (int): Number of agents to train.\n",
    "            state_size (int): Dimension of the state size.\n",
    "            action_size (int): Dimension of the action size.\n",
    "            seed (int): Seed for random.\n",
    "        \"\"\"\n",
    "        critic_input_size = (state_size + action_size) * n_agents\n",
    "\n",
    "        self.actor_regular = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "\n",
    "        self.critic_regular = Critic(critic_input_size, seed).to(device)\n",
    "        self.critic_target = Critic(critic_input_size, seed).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    GOAL = 0.51                 # Environment Goal\n",
    "    AVG_OVER = 100              # Averaged score\n",
    "    N_EPISODES = 3000           # Number of episode for training\n",
    "    MAX_TIMESTEPS = 1000        # Max Timesteps\n",
    "    BUFFER_SIZE = 12000         # Replay Buffer Size\n",
    "    BATCH_SIZE = 256            # Minibatch Size\n",
    "    GAMMA = 0.995               # Discount Gamma\n",
    "    TAU = 1e-3                  # Soft Update Value\n",
    "    LR_ACTOR = 1e-3             # Actor learning rate\n",
    "    LR_CRITIC = 1e-3            # Critic learning rate\n",
    "    UPDATE_EVERY = 2            # Update network every X intervals\n",
    "    N_EXPERIENCES = 4           # Times to learn from batch of experiences\n",
    "    OU_MU = 0.0                 # Noise parameters\n",
    "    OU_SIGMA = 0.2              # Volatility\n",
    "    OU_THETA = 0.15             # Speed of mean reversion\n",
    "    EPSILON = 1                 # Noise exploration epsilon\n",
    "    EPSILON_MIN = 0.01          # Noise exploration minimum\n",
    "    EXPLORATION_STEPS = 12000   # Exploration steps related to N_EXPERIENCES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, agent_id, model, action_size, random_seed):\n",
    "        \"\"\"\n",
    "        Initialize DDPG class.\n",
    "\n",
    "        Args:\n",
    "            agent_id (int): Agent's identifier\n",
    "            model (nn.Module): Module to use.\n",
    "            action_size (int): Dimension of the action size.\n",
    "            seed (int): Seed for random.\n",
    "        \"\"\"\n",
    "        self.id = agent_id\n",
    "\n",
    "        self.actor_regular = model.actor_regular\n",
    "        self.actor_target = model.actor_target\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=Hyperparameters.LR_ACTOR)\n",
    "\n",
    "        self.critic_regular = model.critic_regular\n",
    "        self.critic_target = model.critic_target\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=Hyperparameters.LR_CRITIC)\n",
    "\n",
    "        self.noise = OUNoise(\n",
    "            action_size,\n",
    "            random_seed,\n",
    "            Hyperparameters.OU_MU,\n",
    "            Hyperparameters.OU_THETA,\n",
    "            Hyperparameters.OU_SIGMA,\n",
    "        )\n",
    "\n",
    "        self.deep_copy(self.actor_target, self.actor_regular)\n",
    "        self.deep_copy(self.critic_target, self.critic_regular)\n",
    "\n",
    "    def act(self, states, noise_value, add_noise=True):\n",
    "        \"\"\"\n",
    "        Act upon the give state, and add noise if need be.\n",
    "\n",
    "        Args:\n",
    "            states (nn.Tensor): Tensor of states.\n",
    "            noise_value (float): Amount of noise to add.\n",
    "            add_noise (bool): Flag to add noise.\n",
    "        \"\"\"\n",
    "\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_regular.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_regular(states).cpu().data.numpy()\n",
    "\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        if add_noise:\n",
    "            # Include exploration noise\n",
    "            action += noise_value * self.noise.sample()\n",
    "\n",
    "        # Clip action to the right interval\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def learn(self, agent_id, experiences, all_next_actions, all_actions):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Args:\n",
    "            agent_id (int): Agent's identifier.\n",
    "            experiences (Tuple[torch.Variable]): Tuple of (state, action, reward, next_state, done) tuples\n",
    "            all_next_actions (list): List of all next actions.\n",
    "            all_actions (list): List of all next actions.\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Update the critic neural network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        agent_id = torch.tensor([agent_id]).to(device)\n",
    "        actions_next = torch.cat(all_next_actions, dim=1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        Q_targets = rewards.index_select(1, agent_id) + (Hyperparameters.GAMMA * Q_targets_next * (1 - dones.index_select(1, agent_id)))\n",
    "\n",
    "        loss = torch.nn.MSELoss()\n",
    "        critic_loss = loss(Q_expected, Q_targets.detach())\n",
    "\n",
    "        critic_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_regular.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "\n",
    "        actions_pred = [ actions if i == self.id else actions.detach() for i, actions in enumerate(all_actions) ]\n",
    "        actions_pred = torch.cat(actions_pred, dim=1).to(device)\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        actor_loss.backward()\n",
    "\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_regular, self.critic_target)\n",
    "        self.soft_update(self.actor_regular, self.actor_target)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Soft update model weights.\n",
    "\n",
    "        Note:\n",
    "            θ_target = τ * θ_local + (1 - τ) * θ_target\n",
    "\n",
    "        Args:\n",
    "            local_model (nn.Model): Model to copy the weights from.\n",
    "            target_model (nn.Model): Model to copy the weights to.\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(Hyperparameters.TAU * local_param.data + (1.0 - Hyperparameters.TAU) * target_param.data)\n",
    "\n",
    "    def deep_copy(self, target, source):\n",
    "        \"\"\"\n",
    "        Make a deep copy of weights\n",
    "\n",
    "        Args:\n",
    "            target (nn.Model): Target model.\n",
    "            source (nn.Model): Source model.\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, state_size, action_size, n_agents, random_seed):\n",
    "        \"\"\"\n",
    "        Initialize a MADDPG agent according to the set of states and set of actions from the environment.\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            n_agents (int): Number of agents.\n",
    "            random_seed (int): Seed for random.\n",
    "\n",
    "        Example:\n",
    "            >>> dqn = MADDPG(state_size=37, action_size=4, n_agents=2, random_seed=89)\n",
    "        \"\"\"\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.random_seed = random_seed\n",
    "        self.n_agents = n_agents\n",
    "        self.epsilon = Hyperparameters.EPSILON\n",
    "        self.epsilon_min = Hyperparameters.EPSILON_MIN\n",
    "        self.exploration_steps = Hyperparameters.EXPLORATION_STEPS\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min * Hyperparameters.N_EXPERIENCES) / self.exploration_steps\n",
    "        self.noise_enabled = True\n",
    "        self.timestep_counter = 0\n",
    "        self.agents = self.setup_agents()\n",
    "        self.memory = ReplayBuffer(action_size, Hyperparameters.BUFFER_SIZE, Hyperparameters.BATCH_SIZE, random_seed)\n",
    "\n",
    "    def train(self, env, brain_name):\n",
    "        \"\"\"\n",
    "        Train the agent based on the environment and brain.\n",
    "\n",
    "        Args:\n",
    "            env (UnityEnvironment): Environment to train with.\n",
    "            brain_name (str): Name of the brain.\n",
    "\n",
    "        Example:\n",
    "            >>> maddpg = MADDPG(37, 4, 2, random_seed=89)\n",
    "            >>> maddpg.train(UnityEnvironment(file_name='/path/to/Environment.app'), 'EnvironmentBrain')\n",
    "        \"\"\"\n",
    "        scores_deque = deque(maxlen=Hyperparameters.AVG_OVER)\n",
    "        global_scores = []\n",
    "        averaged_scores = []\n",
    "        max_reward = 0.0\n",
    "\n",
    "        avg_score = None\n",
    "\n",
    "        for i_episode in range(1, Hyperparameters.N_EPISODES+1):\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "\n",
    "            episode_rewards = []\n",
    "\n",
    "            for t in range(Hyperparameters.MAX_TIMESTEPS):\n",
    "                actions = self.act(states)\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                self.step(states, actions, rewards, next_states, dones)\n",
    "                states = next_states\n",
    "                episode_rewards.append(rewards)\n",
    "\n",
    "                if np.any(dones):\n",
    "                    break\n",
    "\n",
    "            episode_reward = np.max(np.sum(np.array(episode_rewards), axis=0))\n",
    "\n",
    "            scores_deque.append(episode_reward)\n",
    "            global_scores.append(episode_reward)\n",
    "            avg_score = np.mean(scores_deque)\n",
    "\n",
    "            averaged_scores.append(avg_score)\n",
    "\n",
    "            if episode_reward > max_reward:\n",
    "                max_reward = episode_reward\n",
    "\n",
    "            print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}', end=\"\")\n",
    "\n",
    "            if i_episode % 50 == 0:\n",
    "                print(f'\\rEpisode {i_episode}\\t'\n",
    "                      f'Average Score: {avg_score:.2f}\\t'\n",
    "                      f'Max Reward: {max_reward:.3f}\\t'\n",
    "                      f'Buffer: {(len(self.memory)/Hyperparameters.BUFFER_SIZE):.3f}\\t'\n",
    "                      f'Noise: {self.epsilon:.3f}\\t'\n",
    "                      f'Timestep: {self.timestep_counter}')\n",
    "\n",
    "            if avg_score >= Hyperparameters.GOAL:\n",
    "                print(f'\\nEnvironment solved in {i_episode:d} episodes!\\tAverage Score: {avg_score:.2f}')\n",
    "                self.checkpoint()\n",
    "                break\n",
    "\n",
    "        return global_scores, avg_score\n",
    "\n",
    "    def test(self, env, brain_name):\n",
    "        \"\"\"\n",
    "        Test the agent based on the environment and brain.\n",
    "\n",
    "        Args:\n",
    "            env (UnityEnvironment): Environment to train with.\n",
    "            brain_name (str): Name of the brain.\n",
    "\n",
    "        Example:\n",
    "            >>> dqn = MADDPG(37, 4, 2, random_seed=89)\n",
    "            >>> dqn.test(UnityEnvironment(file_name='/path/to/Environment.app'), 'EnvironmentBrain')\n",
    "        \"\"\"\n",
    "\n",
    "        self.load_weights()\n",
    "\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "        while True:\n",
    "            actions = self.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            states = next_states\n",
    "\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "    def setup_agents(self):\n",
    "        \"\"\"\n",
    "        Create DDPG agents.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            DDPG(i, ActorCritic(self.n_agents, self.state_size, self.action_size, self.random_seed), self.action_size, self.random_seed)\n",
    "            for i in range(self.n_agents)\n",
    "        ]\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Perform a step in the training process.\n",
    "\n",
    "        Note:\n",
    "            Firstly, saves the state in the replay experience buffer (state, action, reward, next_state, done).\n",
    "            If there are enough numbers of samples in the memory, just sample one of them randomly, and help\n",
    "            the learning process with it.\n",
    "\n",
    "        Args:\n",
    "            states (numpy.array): The current state from which an action has been selected.\n",
    "            actions (numpy.array): The action selected based on the current state.\n",
    "            rewards (float): The reward obtained after applying the action.\n",
    "            next_states (numpy.array): The next state to which the environment will transition after applying the action.\n",
    "            dones (numpy.array): An array of bool flags indicating agents are done.\n",
    "        \"\"\"\n",
    "        states = states.reshape(1, -1)\n",
    "        next_states = next_states.reshape(1, -1)\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "        self.timestep_counter = self.timestep_counter + 1\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > Hyperparameters.BATCH_SIZE and self.timestep_counter % Hyperparameters.UPDATE_EVERY == 0:\n",
    "            for _ in range(Hyperparameters.N_EXPERIENCES):\n",
    "                experiences = [self.memory.sample() for _ in range(self.n_agents)]\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"\n",
    "        Returns actions for given state as per current policy.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array): current state\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for agent, state in zip(self.agents, states):\n",
    "            action = agent.act(state, noise_value=self.epsilon, add_noise=self.noise_enabled)\n",
    "            actions.append(action)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        return np.array(actions).reshape(1, -1)\n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"\n",
    "        Save actor and critic weights for each agent\n",
    "        \"\"\"\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            torch.save(agent.actor_regular.state_dict(),  \"actor_agent_{}.pth\".format(i))\n",
    "            torch.save(agent.critic_regular.state_dict(), \"critic_agent_{}.pth\".format(i))\n",
    "\n",
    "    def load_weights(self):\n",
    "        \"\"\"\n",
    "        Load weights from files.\n",
    "        \"\"\"\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.actor_regular.load_state_dict(torch.load(\"actor_agent_{}.pth\".format(i)))\n",
    "            agent.critic_regular.load_state_dict(torch.load(\"critic_agent_{}.pth\".format(i)))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples for each agent.\n",
    "\n",
    "        Args:\n",
    "            experiences (Tuple[torch.Variable]): Tuple of (state, action, reward, next_state, done) tuples\n",
    "        \"\"\"\n",
    "        next_actions = []\n",
    "        actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            states, _ , _ , next_states, _ = experiences[i]\n",
    "            agent_id = torch.tensor([i]).to(device)\n",
    "\n",
    "            state = states.reshape(-1, self.action_size, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "            action = agent.actor_regular(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            next_state = next_states.reshape(-1, self.action_size, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "            next_action = agent.actor_target(next_state)\n",
    "            next_actions.append(next_action)\n",
    "\n",
    "        # Call to the method learn for each agent using\n",
    "        # the related experiences and all actions/next actions\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.learn(self.memory, i, experiences[i], next_actions, actions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def start(unity):\n",
    "        \"\"\"\n",
    "        Start training.\n",
    "\n",
    "        Note:\n",
    "            Train mode should be enabled.\n",
    "\n",
    "        Args:\n",
    "            unity (Unity): Environment to train with.\n",
    "        \"\"\"\n",
    "        env_info = unity.env.reset(train_mode=True)[unity.brain_name]\n",
    "\n",
    "        action_size = unity.brain.vector_action_space_size\n",
    "\n",
    "        state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "        agent = MADDPG(state_size, action_size, len(env_info.agents), random_seed=89)\n",
    "        scores, avg_score = agent.train(unity.env, unity.brain_name)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.plot(np.arange(len(scores)), [avg_score] * len(scores))\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        ax.legend(['Score', 'Average Score'])\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.00\tMax Reward: 0.000\tBuffer: 0.01175\tNoise: 0.989\tTimestep: 141\n",
      "Episode 20\tAverage Score: 0.00\tMax Reward: 0.000\tBuffer: 0.0235\tNoise: 0.977\tTimestep: 282\n",
      "Episode 30\tAverage Score: 0.00\tMax Reward: 0.000\tBuffer: 0.035333333333333335\tNoise: 0.966\tTimestep: 424\n",
      "Episode 40\tAverage Score: 0.00\tMax Reward: 0.090\tBuffer: 0.0485\tNoise: 0.953\tTimestep: 582\n",
      "Episode 50\tAverage Score: 0.00\tMax Reward: 0.100\tBuffer: 0.06183333333333333\tNoise: 0.941\tTimestep: 742\n",
      "Episode 60\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.08083333333333333\tNoise: 0.922\tTimestep: 970\n",
      "Episode 70\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.09541666666666666\tNoise: 0.908\tTimestep: 1145\n",
      "Episode 80\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.10966666666666666\tNoise: 0.895\tTimestep: 1316\n",
      "Episode 90\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.12291666666666666\tNoise: 0.882\tTimestep: 1475\n",
      "Episode 100\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.1375\tNoise: 0.868\tTimestep: 1650\n",
      "Episode 110\tAverage Score: 0.01\tMax Reward: 0.100\tBuffer: 0.1555\tNoise: 0.851\tTimestep: 1866\n",
      "Episode 120\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.1715\tNoise: 0.835\tTimestep: 2058\n",
      "Episode 130\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.19133333333333333\tNoise: 0.816\tTimestep: 2296\n",
      "Episode 140\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.20858333333333334\tNoise: 0.800\tTimestep: 2503\n",
      "Episode 150\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.22433333333333333\tNoise: 0.785\tTimestep: 2692\n",
      "Episode 160\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.23825\tNoise: 0.771\tTimestep: 2859\n",
      "Episode 170\tAverage Score: 0.02\tMax Reward: 0.100\tBuffer: 0.25275\tNoise: 0.757\tTimestep: 3033\n",
      "Episode 180\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.27266666666666667\tNoise: 0.738\tTimestep: 3272\n",
      "Episode 190\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.293\tNoise: 0.719\tTimestep: 3516\n",
      "Episode 200\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.3124166666666667\tNoise: 0.700\tTimestep: 3749\n",
      "Episode 210\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.32825\tNoise: 0.685\tTimestep: 3939\n",
      "Episode 220\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.347\tNoise: 0.667\tTimestep: 4164\n",
      "Episode 230\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.36291666666666667\tNoise: 0.652\tTimestep: 4355\n",
      "Episode 240\tAverage Score: 0.03\tMax Reward: 0.100\tBuffer: 0.38\tNoise: 0.635\tTimestep: 4560\n",
      "Episode 250\tAverage Score: 0.04\tMax Reward: 0.190\tBuffer: 0.4023333333333333\tNoise: 0.614\tTimestep: 4828\n",
      "Episode 260\tAverage Score: 0.04\tMax Reward: 0.190\tBuffer: 0.42733333333333334\tNoise: 0.590\tTimestep: 5128\n",
      "Episode 270\tAverage Score: 0.05\tMax Reward: 0.190\tBuffer: 0.4483333333333333\tNoise: 0.570\tTimestep: 5380\n",
      "Episode 280\tAverage Score: 0.05\tMax Reward: 0.190\tBuffer: 0.4735\tNoise: 0.545\tTimestep: 5682\n",
      "Episode 290\tAverage Score: 0.05\tMax Reward: 0.190\tBuffer: 0.49466666666666664\tNoise: 0.525\tTimestep: 5936\n",
      "Episode 300\tAverage Score: 0.05\tMax Reward: 0.190\tBuffer: 0.525\tNoise: 0.496\tTimestep: 6300\n",
      "Episode 310\tAverage Score: 0.06\tMax Reward: 0.200\tBuffer: 0.5568333333333333\tNoise: 0.465\tTimestep: 6682\n",
      "Episode 320\tAverage Score: 0.06\tMax Reward: 0.200\tBuffer: 0.5849166666666666\tNoise: 0.438\tTimestep: 7019\n",
      "Episode 330\tAverage Score: 0.06\tMax Reward: 0.200\tBuffer: 0.6070833333333333\tNoise: 0.417\tTimestep: 7285\n",
      "Episode 340\tAverage Score: 0.06\tMax Reward: 0.200\tBuffer: 0.6323333333333333\tNoise: 0.393\tTimestep: 7588\n",
      "Episode 350\tAverage Score: 0.07\tMax Reward: 0.200\tBuffer: 0.6635\tNoise: 0.363\tTimestep: 7962\n",
      "Episode 360\tAverage Score: 0.07\tMax Reward: 0.200\tBuffer: 0.6874166666666667\tNoise: 0.340\tTimestep: 8249\n",
      "Episode 370\tAverage Score: 0.08\tMax Reward: 0.300\tBuffer: 0.7345\tNoise: 0.295\tTimestep: 8814\n",
      "Episode 380\tAverage Score: 0.08\tMax Reward: 0.300\tBuffer: 0.7745833333333333\tNoise: 0.256\tTimestep: 9295\n",
      "Episode 390\tAverage Score: 0.09\tMax Reward: 0.300\tBuffer: 0.8029166666666666\tNoise: 0.229\tTimestep: 9635\n",
      "Episode 400\tAverage Score: 0.09\tMax Reward: 0.300\tBuffer: 0.8319166666666666\tNoise: 0.201\tTimestep: 9983\n",
      "Episode 410\tAverage Score: 0.09\tMax Reward: 0.400\tBuffer: 0.8766666666666667\tNoise: 0.158\tTimestep: 10520\n",
      "Episode 420\tAverage Score: 0.10\tMax Reward: 0.400\tBuffer: 0.9273333333333333\tNoise: 0.110\tTimestep: 11128\n",
      "Episode 430\tAverage Score: 0.12\tMax Reward: 0.400\tBuffer: 0.9738333333333333\tNoise: 0.065\tTimestep: 11686\n",
      "Episode 440\tAverage Score: 0.13\tMax Reward: 0.400\tBuffer: 1.0\tNoise: 0.020\tTimestep: 12247\n",
      "Episode 450\tAverage Score: 0.12\tMax Reward: 0.400\tBuffer: 1.0\tNoise: 0.010\tTimestep: 12501\n",
      "Episode 460\tAverage Score: 0.13\tMax Reward: 0.400\tBuffer: 1.0\tNoise: 0.010\tTimestep: 12864\n",
      "Episode 470\tAverage Score: 0.12\tMax Reward: 0.400\tBuffer: 1.0\tNoise: 0.010\tTimestep: 13152\n",
      "Episode 480\tAverage Score: 0.12\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 13631\n",
      "Episode 490\tAverage Score: 0.12\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 14107\n",
      "Episode 500\tAverage Score: 0.13\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 14690\n",
      "Episode 510\tAverage Score: 0.13\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 15211\n",
      "Episode 520\tAverage Score: 0.12\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 15716\n",
      "Episode 530\tAverage Score: 0.12\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 16120\n",
      "Episode 540\tAverage Score: 0.12\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 16693\n",
      "Episode 550\tAverage Score: 0.13\tMax Reward: 0.500\tBuffer: 1.0\tNoise: 0.010\tTimestep: 17378\n",
      "Episode 560\tAverage Score: 0.14\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 18323\n",
      "Episode 570\tAverage Score: 0.14\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 18863\n",
      "Episode 580\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 19726\n",
      "Episode 590\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 20249\n",
      "Episode 600\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 20741\n",
      "Episode 610\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 21197\n",
      "Episode 620\tAverage Score: 0.16\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 22089\n",
      "Episode 630\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 22542\n",
      "Episode 640\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 22867\n",
      "Episode 650\tAverage Score: 0.15\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 23600\n",
      "Episode 660\tAverage Score: 0.13\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 24054\n",
      "Episode 670\tAverage Score: 0.14\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 25057\n",
      "Episode 680\tAverage Score: 0.14\tMax Reward: 0.800\tBuffer: 1.0\tNoise: 0.010\tTimestep: 25839\n",
      "Episode 690\tAverage Score: 0.18\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 27672\n",
      "Episode 700\tAverage Score: 0.20\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 29168\n",
      "Episode 710\tAverage Score: 0.22\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 30197\n",
      "Episode 720\tAverage Score: 0.22\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 31224\n",
      "Episode 730\tAverage Score: 0.24\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 32094\n",
      "Episode 740\tAverage Score: 0.25\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 33075\n",
      "Episode 750\tAverage Score: 0.25\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 33682\n",
      "Episode 760\tAverage Score: 0.29\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 35541\n",
      "Episode 770\tAverage Score: 0.32\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 37574\n",
      "Episode 780\tAverage Score: 0.32\tMax Reward: 2.300\tBuffer: 1.0\tNoise: 0.010\tTimestep: 38532\n",
      "Episode 790\tAverage Score: 0.35\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 41330\n",
      "Episode 800\tAverage Score: 0.33\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 42090\n",
      "Episode 810\tAverage Score: 0.34\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 43544\n",
      "Episode 820\tAverage Score: 0.37\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 45659\n",
      "Episode 830\tAverage Score: 0.41\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 48067\n",
      "Episode 840\tAverage Score: 0.40\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 48904\n",
      "Episode 850\tAverage Score: 0.40\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 49243\n",
      "Episode 860\tAverage Score: 0.37\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 50262\n",
      "Episode 870\tAverage Score: 0.34\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 51263\n",
      "Episode 880\tAverage Score: 0.41\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 54581\n",
      "Episode 890\tAverage Score: 0.36\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 55766\n",
      "Episode 900\tAverage Score: 0.36\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 56215\n",
      "Episode 910\tAverage Score: 0.36\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 57703\n",
      "Episode 920\tAverage Score: 0.33\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 58675\n",
      "Episode 930\tAverage Score: 0.27\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 58976\n",
      "Episode 940\tAverage Score: 0.28\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 59907\n",
      "Episode 950\tAverage Score: 0.30\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 61010\n",
      "Episode 960\tAverage Score: 0.29\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 61818\n",
      "Episode 970\tAverage Score: 0.28\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 62477\n",
      "Episode 980\tAverage Score: 0.21\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 63130\n",
      "Episode 990\tAverage Score: 0.21\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 64027\n",
      "Episode 1000\tAverage Score: 0.25\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 66090\n",
      "Episode 1010\tAverage Score: 0.26\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 68161\n",
      "Episode 1020\tAverage Score: 0.30\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 70596\n",
      "Episode 1030\tAverage Score: 0.34\tMax Reward: 2.600\tBuffer: 1.0\tNoise: 0.010\tTimestep: 72420\n",
      "Episode 1040\tAverage Score: 0.40\tMax Reward: 2.700\tBuffer: 1.0\tNoise: 0.010\tTimestep: 75903\n",
      "Episode 1050\tAverage Score: 0.43\tMax Reward: 2.700\tBuffer: 1.0\tNoise: 0.010\tTimestep: 78011\n",
      "Episode 1060\tAverage Score: 0.44\tMax Reward: 2.700\tBuffer: 1.0\tNoise: 0.010\tTimestep: 79315\n",
      "Episode 1070\tAverage Score: 0.48\tMax Reward: 2.700\tBuffer: 1.0\tNoise: 0.010\tTimestep: 81411\n",
      "Episode 1080\tAverage Score: 0.51\tMax Reward: 2.700\tBuffer: 1.0\tNoise: 0.010\tTimestep: 83302\n",
      "Episode 1081\tAverage Score: 0.51\n",
      "Environment solved in 1081 episodes!\tAverage Score: 0.51\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Training.start(unity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def start(unity):\n",
    "        \"\"\"\n",
    "        Start evaluating.\n",
    "\n",
    "        Note:\n",
    "            Train mode should be disabled.\n",
    "\n",
    "        Args:\n",
    "            unity (Unity): Environment wrapper.\n",
    "        \"\"\"\n",
    "        env_info = unity.env.reset(train_mode=False)[unity.brain_name]\n",
    "        action_size = unity.brain.vector_action_space_size\n",
    "        state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "        agent = MADDPG(state_size, action_size, len(env_info.agents), random_seed=89)\n",
    "        agent.test(unity.env, unity.brain_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "Evaluation.start(unity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}